# Hidden-Markov-Models

A Hidden Markov Model (HMM) is a statistical model that is often used to model a sequence of observations that are generated by a sequence of states. The states of the HMM are not directly observable, hence the name "hidden".

The key idea behind the HMM is that each state generates a set of observations, and the model defines the probability of transitioning between states, as well as the probability of observing a certain sequence of observations given the underlying sequence of states.

The mathematical formulas for an HMM are as follows:

- Initial state probabilities:
Ï€_i = P(X_1 = i)

- State transition probabilities:
a_{i,j} = P(X_t = j | X_{t-1} = i)

- Observation probabilities:
b_j(k) = P(Y_t = k | X_t = j)

- The forward variable, alpha, is defined as the probability of observing the sequence up to time t and being in state i at time t:
alpha_{t,i} = P(Y_1, Y_2, ..., Y_t, X_t = i)

- The backward variable, beta, is defined as the probability of observing the sequence from time t+1 to the end given that we're in state i at time t:
beta_{t,i} = P(Y_{t+1}, Y_{t+2}, ..., Y_T | X_t = i)

- The most likely sequence of hidden states given the observations is found using the Viterbi algorithm. The Viterbi algorithm is a dynamic programming algorithm that maximizes the probability of the most likely sequence of states given the observations.

Here's an example of how to implement the forward-backward algorithm in C++ to calculate the most likely sequence of hidden states given the observations:

```
#include <iostream>
#include <vector>
using namespace std;

const int N = 2; // Number of states
const int T = 3; // Number of observations

double pi[N] = {0.5, 0.5};
double a[N][N] = {{0.7, 0.3}, {0.4, 0.6}};
double b[N][T] = {{0.1, 0.4, 0.5}, {0.6, 0.3, 0.1}};
int observations[T] = {0, 1, 0};

vector<vector<double>> alpha(T, vector<double>(N));
vector<vector<double>> beta(T, vector<double>(N));
vector<int> q(T);

void forward(){
    // Initialize alpha
    for (int i = 0; i < N; i++) {
        alpha[0][i] = pi[i] * b[i][observations[0]];
    }
    // Compute alpha
    for (int t = 1; t < T; t++) {
        for (int i = 0; i < N; i++) {
            double sum = 0;
            for (int j = 0; j < N; j++) {
                sum += alpha[t-1][j] * a[j][i];
            }
            alpha[t][i] = sum * b[i][observations[t]];
        }
    }
}

void backward(){
    // Initialize beta
    for (int i = 0; i < N; i++) {
        beta[T-1][i] = 1;
    }
    // Compute beta
    for (int t = T-2; t >= 0; t--) {
        for (int i = 0; i < N; i++) {
            double sum = 0;
            for (int j = 0; j < N; j++) {
                sum += beta[t+1][j] * a[i][j] * b[j][observations[t+1]];
            }
            beta[t][i] = sum;
        }
    }
}

void viterbi(){
    // Compute the most likely sequence of hidden states
    for (int t = 0; t < T; t++) {
        double maxProb = 0;
        int maxState = 0;
        for (int i = 0; i < N; i++) {
            double prob = alpha[t][i] * beta[t][i];
            if (prob > maxProb) {
                maxProb = prob;
                maxState = i;
            }
        }
        q[t] = maxState;
    }
}

int main() {
    forward();
    backward();
    viterbi();
    // Print the most likely sequence of hidden states
    cout << "Most likely sequence of hidden states:" << endl;
    for (int i = 0 = 0; i < T; i++) {
cout << q[i] << " ";
}
return 0;
}
```

This code defines an HMM with 2 states and 3 observations, and uses the forward-backward algorithm to calculate the most likely sequence of hidden states given the observations. The algorithm uses the probability of being in a certain state given the observations at time t, and the probability of the observations from time t+1 to the end of the sequence given that we're in a certain state at time t. Finally, the most likely sequence of hidden states is calculated using the Viterbi algorithm. The final result is printed out in the end.

The code uses three functions, forward() to implement the forward algorithm, backward() to implement the backward algorithm and viterbi() to implement the Viterbi algorithm, Each function is called in main function and the final result is printed out in the end.

It's important to note that the above code is just an example and the HMM model is not trained and the observations and states are pre-defined. In a real-world scenario, you would need to train the model with a dataset of observations and states.

